{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1aa606bc",
   "metadata": {},
   "source": [
    "# Analyzing Subjects:\n",
    "<break> </break>\n",
    "<font size=4>\n",
    "The Analyzer, along with the classifier class, provide lots of functionality towards analyzing the Cool Neighbors classification data.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c417ecae",
   "metadata": {},
   "source": [
    "## Aggregating the Classifications\n",
    "\n",
    "The Analyzer class object requires the extraction and reduction files produced via the Aggregator class. See the Aggregating example for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59e062a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated files already exist, skipping aggregation.\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import os\n",
    "from DataToolkit.Aggregator import Aggregator\n",
    "\n",
    "# For more details about Aggregating, see the Aggregating example.\n",
    "\n",
    "# This is the workflow ID and version for the Backyard Worlds: Cool Neighbors project's Launch-0 workflow.\n",
    "workflow_id = 24299\n",
    "version = 1.6\n",
    "\n",
    "# Default names for the CSV files that are exported via Zooniverse's data exports tab.\n",
    "classifications_csv = \"backyard-worlds-cool-neighbors-classifications.csv\"\n",
    "workflows_csv = \"backyard-worlds-cool-neighbors-workflows.csv\"\n",
    "config_directory = \"Config\"\n",
    "extractions_directory = \"Extractions\"\n",
    "reductions_directory = \"Reductions\"\n",
    "\n",
    "# Check whether the aggregated files already exist for this workflow and version\n",
    "aggregator = Aggregator(classifications_csv_filename=classifications_csv, workflow_csv_filename=workflows_csv, config_directory=config_directory, extractions_directory=extractions_directory, reductions_directory=reductions_directory)\n",
    "\n",
    "if(os.path.exists(\"{}/question_extractor_workflow_{}_V{}.csv\".format(aggregator.extractions_directory, workflow_id, version)) and os.path.exists(\"{}/question_reducer_workflow_{}_V{}.csv\".format(aggregator.reductions_directory, workflow_id, version))):\n",
    "    print(\"Aggregated files already exist, skipping aggregation.\")\n",
    "else:\n",
    "    print(\"Aggregating...\")\n",
    "    aggregator.aggregateWorkflow(workflow_id=workflow_id, v=version)\n",
    "    print(\"Aggregation complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b00a8ef",
   "metadata": {},
   "source": [
    "## Creating the Analyzer\n",
    "The analyzer will be your all-inclusive tool for working with the panoptes_aggregation results, generated via the Aggregator.\n",
    "\n",
    "In addition to the analzyer's functionality, it also has a Classifier instance within it which itself has functionality regarding the accuracies of user's and weighting classifications by user accuracy in the candidate selection process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "395bdfd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Analyzer...\n",
      "Loaded Analyzer object from 'analyzer.pickle'\n"
     ]
    }
   ],
   "source": [
    "# Import\n",
    "from DataToolkit.Analyzer import Analyzer\n",
    "\n",
    "# Provide the filepaths of the aggregated files\n",
    "extracted_file = \"Extractions/question_extractor_workflow_24299_V1.6.csv\"\n",
    "reduced_file = \"Reductions/question_reducer_workflow_24299_V1.6.csv\"\n",
    "\n",
    "# Subject file is optional but highly recommended, as it allows for you to work with subjects offline\n",
    "# and is generally faster than the online version.\n",
    "subject_file = \"backyard-worlds-cool-neighbors-subjects.csv\"\n",
    "if(subject_file is not None):\n",
    "    # If an offline analyzer has already been created and saved, you can load it instead of creating it again. You cannot\n",
    "    # load an online analyzer.\n",
    "    if (os.path.exists(\"analyzer.pickle\")):\n",
    "        print(\"Loading Analyzer...\")\n",
    "        analyzer = Analyzer.load()\n",
    "    else:\n",
    "        print(\"Creating Analyzer...\")\n",
    "        # Providing a subjects_file will default the analyzer to being offline.\n",
    "        analyzer = Analyzer(extracted_file, reduced_file, subject_file)\n",
    "else:\n",
    "    # Not providing a subjects_file will default the analyzer to being online.\n",
    "    analyzer = Analyzer(extracted_file, reduced_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3162ed0",
   "metadata": {},
   "source": [
    "## Getting Information\n",
    "\n",
    "Lots of different types of information can be extracted out of the classification data. Provided below are useful catgegorical examples:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb7fe3b",
   "metadata": {},
   "source": [
    "### Getting Subjects and Users\n",
    "Subject ids, usernames, and user ids are able to be retrieved via the Analyzer as well as their panoptes-client object equivalents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e138153c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Subjects: 89099782 89099844 89099869 89100067 89100074 89100144 89100157 89100208 89100248 89100284 ...\n",
      "\n",
      "Usernames: robbinsg NoahSchapera.9 pga99 not-logged-in-f6496faa335471e8e9d4 Albionaa Night-Vega not-logged-in-99db418729914db69dc7 Herakles Marcossilva vinnyzo ...\n",
      "\n",
      "User ids: 2129944.0 2475288.0 1728334.0 nan 2161413.0 2335011.0 1572079.0 1589918.0 2624692.0 2582694.0 ...\n",
      "\n",
      "Top usernames: ['ConC', 'Vidar87', 'LizzethRuiz', 'Rattus', 'pga99', 'Borvo', 'BlueWhovian', 'VinodThakur', 'SAlexandrov', 'chulej', 'ShyBay', 'Marcossilva', 'Rosie_Oliver', 'Geert_D', 'nilium', 'wangqintao', 'pathfinder7567', 'ARTEM1', 'BruceHorlyck', 'eantonio2023', 'Barbalbero', 'Herakles', 'jcstew', 'graham_d', 'shocko61', 'Sharp88', 'leelaht', 'canopus228', 'RoksolanaKot', 'Harborist', 'Knightrider95', 'malcolmf', 'EllieSparke', 'Gronagor', 'chrostek', 'thorsteinn', 'Kennycavanaugh', 'MartinKb', 'jiipee', 'Naoufel67', 'Jesskantine', 'BLGoodwin', 'autumnwisp', 'Jose_Campos', 'MrAgent99', 'kiri2ll']\n",
      "\n",
      "Login credentials loaded.\n",
      "\n",
      "Subject object for subject 89099782: <Subject 89099782>\n",
      "\n",
      "User object for user robbinsg: <User 2129944>\n",
      "\n",
      "Top user objects: [<User 1447426>, <User 1370760>, <User 2379602>, <User 1053411>, <User 1728334>, <User 1926942>, <User 612898>, <User 1567167>, <User 2031043>, <User 59809>, <User 1733572>, <User 1589918>, <User 2007701>, <User 1760737>, <User 2947>, <User 2476100>, <User 1858149>, <User 1648678>, <User 2627530>, <User 2628347>, <User 1856>, <User 1572079>, <User 1810058>, <User 1254778>, <User 3427>, <User 1431886>, <User 404505>, <User 1901434>, <User 2389951>, <User 1572111>, <User 1059348>, <User 1619238>, <User 2627124>, <User 1857010>, <User 5216>, <User 656421>, <User 2599524>, <User 2134093>, <User 646>, <User 2558395>, <User 2626335>, <User 1430195>, <User 2077487>, <User 135>, <User 2350547>, <User 1567891>]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import\n",
    "from unWISE_verse.Spout import Spout\n",
    "\n",
    "# Get the valid subject ids from the workflow classifications.\n",
    "subject_ids = analyzer.getSubjectIDs()\n",
    "print(f\"Valid Subjects:\", *subject_ids[0:10], \"...\\n\")\n",
    "\n",
    "# Get the usernames of the users who have classified.\n",
    "usernames = analyzer.getUniqueUserIdentifiers(user_identifier=\"username\")\n",
    "print(f\"Usernames:\", *usernames[0:10], \"...\\n\")\n",
    "\n",
    "# Get the user ids of the users who have classified.\n",
    "# include_logged_out_users must be false since logged-out users do not have user ids.\n",
    "user_ids = analyzer.getUniqueUserIdentifiers(include_logged_out_users=False, user_identifier=\"user id\")\n",
    "print(f\"User ids:\", *user_ids[0:10], \"...\\n\")\n",
    "\n",
    "\n",
    "# Get the top usernames (two modes: percentile or classification threshold).\n",
    "top_usernames = analyzer.getTopUsernames(classification_threshold=None, percentile=98)\n",
    "print(f\"Top usernames: {top_usernames}\\n\")\n",
    "\n",
    "# Login to Zooniverse with Spout to access the next two functions.\n",
    "# You will need to log in to Spout to use these functions or use online mode.\n",
    "login = Spout.requestLogin()\n",
    "Spout.loginToZooniverse(login)\n",
    "print()\n",
    "# Get the subject object for a specific subject. Not disabled for offline mode, but you will need to log in to Spout\n",
    "# to get the subject object.\n",
    "subject_object = analyzer.getSubject(subject_ids[0])\n",
    "print(f\"Subject object for subject {subject_ids[0]}: {subject_object}\\n\")\n",
    "\n",
    "# Get the user object for a specific user. Not disabled for offline mode, but you will need to log in to Spout\n",
    "# to get the user object.\n",
    "user_object = analyzer.getUser(usernames[0])\n",
    "print(f\"User object for user {usernames[0]}: {user_object}\\n\")\n",
    "\n",
    "# Gets the user objects of the top users (two modes: percentile or classification threshold).\n",
    "top_user_objects = analyzer.getTopUsers(classification_threshold=None, percentile=98)\n",
    "print(f\"Top user objects: {top_user_objects}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69501392",
   "metadata": {},
   "source": [
    "### Number of Classifications\n",
    "\n",
    "These functions pertain to getting the total number of classifications of all users, or some specific subset of users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ce06a006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classifications: 346646\n",
      "\n",
      "Number of classifications for subjects with at least 5 classifications: 346496\n",
      "\n",
      "Total classifications by user robbinsg: 120\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the total number of classifications in the aggregated files.\n",
    "print(f\"Number of classifications: {analyzer.getTotalClassifications()}\\n\")\n",
    "\n",
    "# Get the total number of classifications for a subjects which have at least n classifications.\n",
    "n = 5\n",
    "print(f\"Number of classifications for subjects with at least {n} classifications: {analyzer.getSubsetOfTotalClassifications(minimum_subject_classification_count=5)}\\n\")\n",
    "\n",
    "# Get the total number of classifications done by a specific user.\n",
    "user_classification_count = analyzer.getTotalClassificationsByUser(usernames[0])\n",
    "print(f\"Total classifications by user {usernames[0]}: {user_classification_count}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d2ee1f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataToolkit",
   "language": "python",
   "name": "datatoolkit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
