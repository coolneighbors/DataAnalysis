{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1aa606bc",
   "metadata": {},
   "source": [
    "# Analyzing Subjects:\n",
    "<break> </break>\n",
    "<font size=4>\n",
    "The Analyzer, along with the classifier class, provide lots of functionality towards analyzing the Cool Neighbors classification data.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c417ecae",
   "metadata": {},
   "source": [
    "## Aggregating the Classifications\n",
    "\n",
    "The Analyzer class object requires the extraction and reduction files produced via the Aggregator class. See the Aggregating example for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e062a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "from DataToolkit.Aggregator import Aggregator\n",
    "\n",
    "# For more details about Aggregating, see the Aggregating example.\n",
    "\n",
    "# This is the workflow ID and version for the Backyard Worlds: Cool Neighbors project's Launch-0 workflow.\n",
    "workflow_id = 24299\n",
    "version = 1.6\n",
    "\n",
    "# Default names for the CSV files that are exported via Zooniverse's data exports tab.\n",
    "classifications_csv = \"backyard-worlds-cool-neighbors-classifications.csv\"\n",
    "workflows_csv = \"backyard-worlds-cool-neighbors-workflows.csv\"\n",
    "config_directory = \"Config\"\n",
    "extractions_directory = \"Extractions\"\n",
    "reductions_directory = \"Reductions\"\n",
    "\n",
    "# Check whether the aggregated files already exist for this workflow and version\n",
    "aggregator = Aggregator(classifications_csv_filename=classifications_csv, workflow_csv_filename=workflows_csv, config_directory=config_directory, extractions_directory=extractions_directory, reductions_directory=reductions_directory)\n",
    "\n",
    "if(os.path.exists(\"{}/question_extractor_workflow_{}_V{}.csv\".format(aggregator.extractions_directory, workflow_id, version)) and os.path.exists(\"{}/question_reducer_workflow_{}_V{}.csv\".format(aggregator.reductions_directory, workflow_id, version))):\n",
    "    print(\"Aggregated files already exist, skipping aggregation.\")\n",
    "else:\n",
    "    print(\"Aggregating...\")\n",
    "    aggregator.aggregateWorkflow(workflow_id=workflow_id, v=version)\n",
    "    print(\"Aggregation complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b00a8ef",
   "metadata": {},
   "source": [
    "## Creating the Analyzer\n",
    "The analyzer will be your all-inclusive tool for working with the panoptes_aggregation results, generated via the Aggregator.\n",
    "\n",
    "In addition to the analzyer's functionality, it also has a Classifier instance within it which itself has functionality regarding the accuracies of user's and weighting classifications by user accuracy in the candidate selection process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395bdfd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import\n",
    "from DataToolkit.Analyzer import Analyzer\n",
    "\n",
    "# Provide the filepaths of the aggregated files\n",
    "extracted_file = \"Extractions/question_extractor_workflow_24299_V1.6.csv\"\n",
    "reduced_file = \"Reductions/question_reducer_workflow_24299_V1.6.csv\"\n",
    "\n",
    "# Subject file is optional but highly recommended, as it allows for you to work with subjects offline\n",
    "# and is generally faster than the online version.\n",
    "subject_file = \"backyard-worlds-cool-neighbors-subjects.csv\"\n",
    "if(subject_file is not None):\n",
    "    # If an offline analyzer has already been created and saved, you can load it instead of creating it again. You cannot\n",
    "    # load an online analyzer.\n",
    "    if (os.path.exists(\"analyzer.pickle\")):\n",
    "        print(\"Loading Analyzer...\")\n",
    "        analyzer = Analyzer.load()\n",
    "    else:\n",
    "        print(\"Creating Analyzer...\")\n",
    "        # Providing a subjects_file will default the analyzer to being offline.\n",
    "        analyzer = Analyzer(extracted_file, reduced_file, subject_file)\n",
    "else:\n",
    "    # Not providing a subjects_file will default the analyzer to being online.\n",
    "    analyzer = Analyzer(extracted_file, reduced_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3162ed0",
   "metadata": {},
   "source": [
    "## Getting Analyzer Information\n",
    "\n",
    "Lots of different types of information can be extracted out of the classification data. Provided below are useful catgegorical examples:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb7fe3b",
   "metadata": {},
   "source": [
    "### Getting Subjects and Users\n",
    "Subject ids, usernames, and user ids are able to be retrieved via the Analyzer as well as their panoptes-client object equivalents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e138153c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import\n",
    "from unWISE_verse.Spout import Spout\n",
    "\n",
    "# Get the valid subject ids from the workflow classifications.\n",
    "subject_ids = analyzer.getSubjectIDs()\n",
    "print(f\"Valid Subjects:\", *subject_ids[0:10], \"...\\n\")\n",
    "\n",
    "# Get the usernames of the users who have classified.\n",
    "usernames = analyzer.getUniqueUserIdentifiers(user_identifier=\"username\")\n",
    "print(f\"Usernames:\", *usernames[0:10], \"...\\n\")\n",
    "\n",
    "# Get the user ids of the users who have classified.\n",
    "# include_logged_out_users must be false since logged-out users do not have user ids.\n",
    "user_ids = analyzer.getUniqueUserIdentifiers(include_logged_out_users=False, user_identifier=\"user id\")\n",
    "\n",
    "# Get the top usernames (two modes: percentile or classification threshold).\n",
    "top_usernames = analyzer.getTopUsernames(classification_threshold=None, percentile=98)\n",
    "print(f\"Top usernames: {top_usernames}\\n\")\n",
    "\n",
    "# Login to Zooniverse with Spout to access the next two functions.\n",
    "# You will need to log in to Spout to use these functions or use online mode.\n",
    "login = Spout.requestLogin()\n",
    "Spout.loginToZooniverse(login)\n",
    "print()\n",
    "# Get the subject object for a specific subject. Not disabled for offline mode, but you will need to log in to Spout\n",
    "# to get the subject object.\n",
    "subject_object = analyzer.getSubject(subject_ids[0])\n",
    "print(f\"Subject object for subject {subject_ids[0]}: {subject_object}\\n\")\n",
    "\n",
    "# Get the user object for a specific user. Not disabled for offline mode, but you will need to log in to Spout\n",
    "# to get the user object.\n",
    "user_object = analyzer.getUser(usernames[0])\n",
    "print(f\"User object for user {usernames[0]}: {user_object}\\n\")\n",
    "\n",
    "# Gets the user objects of the top users (two modes: percentile or classification threshold).\n",
    "top_user_objects = analyzer.getTopUsers(classification_threshold=None, percentile=98)\n",
    "print(f\"Top user objects: {top_user_objects}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69501392",
   "metadata": {},
   "source": [
    "### Number of Classifications\n",
    "\n",
    "These functions pertain to getting the total number of classifications of all users, or some specific subset of users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce06a006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the total number of classifications in the aggregated files.\n",
    "print(f\"Number of classifications: {analyzer.getTotalClassifications()}\\n\")\n",
    "\n",
    "# Get the total number of classifications for a subjects which have at least n classifications.\n",
    "n = 5\n",
    "print(f\"Number of classifications for subjects with at least {n} classifications: {analyzer.getSubsetOfTotalClassifications(minimum_subject_classification_count=5)}\\n\")\n",
    "\n",
    "# Get the total number of classifications done by a specific user.\n",
    "user_classification_count = analyzer.getTotalClassificationsByUser(usernames[0])\n",
    "print(f\"Total classifications by user {usernames[0]}: {user_classification_count}\\n\")\n",
    "\n",
    "# Get the total number of classifications done by the top users (two modes: percentile or classification threshold).\n",
    "top_users_classification_count = analyzer.getTotalClassificationsByTopUsers(classification_threshold=None, percentile=98)\n",
    "print(f\"Total classifications by top users: {top_users_classification_count}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0521b656",
   "metadata": {},
   "source": [
    "### Classifications\n",
    "These functions allow you to obtain the classification information from specific subjects and specific users. Classifications are represented as a dictionary with keys: \"yes\", \"no\", and \"total\". These correspond to the binary choice users make in the task question on the Cool Neighbors' classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64df87e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the classifications done for a specific subject.\n",
    "subject_classifications = analyzer.getClassificationsForSubject(subject_ids[0])\n",
    "print(f\"Classifications for subject {subject_ids[0]}: {subject_classifications}\\n\")\n",
    "\n",
    "# Get classifications done by a specific user. \n",
    "# Since a user could classify many subjects, a Pandas Dataframe of the classifications is provided.\n",
    "user_classifications = analyzer.getClassificationsByUser(usernames[0])\n",
    "print(f\"Classifications by user {usernames[0]}: \\n{user_classifications}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090ccfc3",
   "metadata": {},
   "source": [
    "### Plotting Classifications\n",
    "Plot classification data in a variety of different and useful ways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fcc9bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the classification distribution for all subjects.\n",
    "total_subject_count = 27800\n",
    "print(\"Plotting classification distribution for all valid subjects...\\n\")\n",
    "analyzer.plotClassificationDistribution(total_subject_count=total_subject_count, title=\"Classification Distribution\")\n",
    "\n",
    "# Plot the classifications done for a specific subject.\n",
    "print(f\"Plotting classifications for subject {subject_ids[0]}...\\n\")\n",
    "analyzer.plotClassificationsForSubject(subject_ids[0])\n",
    "\n",
    "# Plot the classifications done by the top users (two modes: percentile or classification threshold).\n",
    "print(\"Plotting classifications done by top users...\\n\")\n",
    "analyzer.plotTotalClassificationsByTopUsers(classification_threshold=None, percentile=98)\n",
    "\n",
    "# Plot classification timeline\n",
    "print(\"Plotting classification timeline...\\n\")\n",
    "analyzer.plotClassificationTimeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1162ea3c",
   "metadata": {},
   "source": [
    "### Classification Times\n",
    "Plotting information about classification times and statistics related to the classification times of users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39372c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the time histogram for all classifications/users. Since there are users who have too\n",
    "# few consecutive classifications, which brings up a warning, I am ignoring warnings for this function.\n",
    "print(\"Plotting time histogram for all classifications...\\n\")\n",
    "from DataToolkit.Decorators import ignore_warnings \n",
    "# This decorator will ignore the warnings provided by users with insufficient consecutive classifications\n",
    "ignore_warnings(analyzer.plotTimeHistogramForAllClassifications)()\n",
    "\n",
    "# Plot the time histogram for a specific user.\n",
    "print(f\"Plotting time histogram for user {usernames[0]}...\\n\")\n",
    "analyzer.plotTimeHistogramForUserClassifications(usernames[0])\n",
    "\n",
    "# Compute the time statistics for a specific user.\n",
    "user_average_time, user_std_time, user_median_time = analyzer.computeTimeStatisticsForUser(usernames[0])\n",
    "print(f\"Average time for user {usernames[0]}: {round(user_average_time,2)} seconds\\nStandard deviation: {round(user_std_time,2)} seconds\\nMedian: {round(user_median_time,2)} seconds\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09cd4e4c",
   "metadata": {},
   "source": [
    "### Subject Dataframes\n",
    "All csv file information is saved in the Analyzer object as Pandas Dataframes. There are three different csv sources: the subjects file, the extracted file, and the reduced file. \n",
    "\n",
    "-The subjects file is the exported subject csv from the Data Exports tab of Zooniverse, this is provided when using the Analyzer in offline mode. However, you can still generate subject dataframes in the online version as it will gather all the metadata information from the subject objects it gathered upon creation.\n",
    "\n",
    "-The extracted file is the individual classifications produced via the Aggregator. This contains information regarding every classification of every subject by the user who performed the classification.\n",
    "\n",
    "-The reduced file is the combined classifications produced via the Aggregator. This contains the combined information regarding the total number of \"yes\" classifcations and \"no\" classifications by all the users for each individual subject.\n",
    "\n",
    "This information is accessible as needed via the following functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49ff428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the subject dataframe for a specific subject.\n",
    "subject_dataframe_from_subject_file = analyzer.getSubjectDataframe(subject_ids[0], dataframe_type=\"default\")\n",
    "print(f\"Default Subject dataframe for subject {subject_ids[0]}: \\n{subject_dataframe_from_subject_file}\\n\")\n",
    "\n",
    "# Get the subject dataframe for a specific subject from the extracted file.\n",
    "subject_dataframe_from_extracted_file = analyzer.getSubjectDataframe(subject_ids[0], dataframe_type=\"extracted\")\n",
    "print(f\"Extracted Subject dataframe for subject {subject_ids[0]}: \\n{subject_dataframe_from_extracted_file}\\n\")\n",
    "\n",
    "# Get the subject dataframe for a specific subject from the reduced file.\n",
    "subject_dataframe_from_reduced_file = analyzer.getSubjectDataframe(subject_ids[0], dataframe_type=\"reduced\")\n",
    "print(f\"Reduced Subject dataframe for subject {subject_ids[0]}: \\n{subject_dataframe_from_reduced_file}\\n\")\n",
    "\n",
    "# Combine subject dataframes.\n",
    "subject_dataframe_0 = analyzer.getSubjectDataframe(subject_ids[0], dataframe_type=\"default\")\n",
    "subject_dataframe_1 = analyzer.getSubjectDataframe(subject_ids[1], dataframe_type=\"default\")\n",
    "combined_subject_dataframe = analyzer.combineSubjectDataframes([subject_dataframe_0, subject_dataframe_1])\n",
    "print(f\"Combined subject dataframe: \\n{combined_subject_dataframe}\\n\")\n",
    "\n",
    "# Save the subject dataframe to a CSV file.\n",
    "print(\"Saving subject dataframe to file...\")\n",
    "analyzer.saveSubjectDataframeToFile(combined_subject_dataframe, \"combined_subject_dataframe.csv\")\n",
    "print(\"Subject dataframe saved. \\n\")\n",
    "\n",
    "# Load the subject dataframe from a CSV file.\n",
    "combined_subject_dataframe_from_file = analyzer.loadSubjectDataframeFromFile(\"combined_subject_dataframe.csv\")\n",
    "print(f\"Combined subject dataframe from file: \\n{combined_subject_dataframe_from_file}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc14000",
   "metadata": {},
   "source": [
    "### Subject Information\n",
    "Access subject metadata to perform actions on the subjects as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87b9671",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that a subject exists within the Analyzer.\n",
    "subject_exists = analyzer.subjectExists(subject_ids[0])\n",
    "print(f\"Subject {subject_ids[0]} exists within the Analyzer: {subject_exists} \\n\")\n",
    "\n",
    "# Get the subject metadata for a specific subject.\n",
    "subject_metadata = analyzer.getSubjectMetadata(subject_ids[0])\n",
    "print(f\"Subject metadata for subject {subject_ids[0]}: \\n{subject_metadata}\\n\")\n",
    "\n",
    "# Get a particular subject metadata field for a specific subject.\n",
    "subject_metadata_field = analyzer.getSubjectMetadataField(subject_ids[0], \"ID\")\n",
    "print(f\"Subject metadata field for subject {subject_ids[0]}: {subject_metadata_field}\\n\")\n",
    "\n",
    "# Show the subject in wise-view. \n",
    "# open_in_browser = True enforces a delay of 10 seconds before finishing to avoid accidently spamming WiseView\n",
    "print(f\"Showing subject {subject_ids[0]} in wise-view...\\n\")\n",
    "print(f\"WiseView link for subject {subject_ids[0]}: {analyzer.showSubjectInWiseView(subject_ids[0], open_in_browser=True)}\\n\")\n",
    "\n",
    "# Get the SIMBAD link for a specific subject.\n",
    "simbad_link = analyzer.getSimbadLinkForSubject(subject_ids[0])\n",
    "print(f\"SIMBAD link for subject {subject_ids[0]}: {simbad_link}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690f4b51",
   "metadata": {},
   "source": [
    "## Running Queries\n",
    "\n",
    "The Analyzer object also has built in functionality, via the Searcher class, to allow for queries from both SIMBAD and Gaia for each subject's coordinates and field of view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3365a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: If you run these queries with plot=True in standard Python, you can interact with the plot and hover over points \n",
    "# to tell you their given designations in SIMBAD\n",
    "\n",
    "# Get the subject IDs for the queries.\n",
    "subject_ids = analyzer.getSubjectIDs()\n",
    "\n",
    "# Get the SIMBAD query for a specific subject.\n",
    "simbad_query = analyzer.getSimbadQueryForSubject(subject_ids[0], plot=True)\n",
    "print(f\"SIMBAD query for subject {subject_ids[0]}: \\n{simbad_query}\\n\")\n",
    "\n",
    "# Get the conditional SIMBAD query for a specific subject.\n",
    "conditional_simbad_query = analyzer.getConditionalSimbadQueryForSubject(subject_ids[0], plot=True)\n",
    "print(f\"Conditional SIMBAD query for subject {subject_ids[0]}: \\n{conditional_simbad_query}\\n\")\n",
    "\n",
    "# Check if there exists a source in SIMBAD for a specific subject's FOV.\n",
    "source_exists_in_simbad = analyzer.sourceExistsInSimbadForSubject(subject_ids[0])\n",
    "print(f\"Source exists in SIMBAD for subject {subject_ids[0]}: {source_exists_in_simbad}\\n\")\n",
    "\n",
    "# Get the Gaia query for a specific subject.\n",
    "gaia_query = analyzer.getGaiaQueryForSubject(subject_ids[0], plot=True)\n",
    "print(f\"Gaia query for subject {subject_ids[0]}: \\n{gaia_query}\\n\")\n",
    "\n",
    "# Get the conditional Gaia query for a specific subject.\n",
    "conditional_gaia_query = analyzer.getConditionalGaiaQueryForSubject(subject_ids[0], plot=True)\n",
    "print(f\"Conditional Gaia query for subject {subject_ids[0]}: \\n{conditional_gaia_query}\\n\")\n",
    "\n",
    "# Check if there exists a source in Gaia for a specific subject's FOV.\n",
    "source_exists_in_gaia = analyzer.sourceExistsInGaiaForSubject(subject_ids[0])\n",
    "print(f\"Source exists in Gaia for subject {subject_ids[0]}: {source_exists_in_gaia}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8f98f0",
   "metadata": {},
   "source": [
    "## Finding Candidates\n",
    "The Analyzer can perform two distinct actions in regards to finding acceptable Cool Neighbors candidates.\n",
    "\n",
    "Firstly, it can go through all the subjects and determine what subset of the subjects satisfy two conditions:\n",
    "- Has an acceptance ratio (ratio of yes classifications to the total number of classifications) greater than or equal to the minimum acceptance ratio.\n",
    "- Has an acceptance threshold (number of yes classifications) greater than or equal to the minimum acceptance threshold.\n",
    "\n",
    "How these values are determined per subject can be modified such that a weighted version of a user's classifications is taken into account and is incorporated into the overall acceptance ratio and acceptance threshold of each subject. This is not active by default.\n",
    "\n",
    "All candidates which satisfy these conditions will be put into an acceptable candidates list for examination or for further scrutinizing.\n",
    "\n",
    "Secondly, it can go through each acceptable candidate and determine if there is a source within either SIMBAD or Gaia which most likely matches up with the subject. If there is not such a source, it will be added to a csv file. Otherwise it will disregard the subject.\n",
    "- For SIMBAD, the criteria for matching is that there needs to be a source within a field of view (centered on the subject's coordinates) of 120 arcseconds, plus some extra FOV to account for high proper motion objects, which has an otype of any of the following: BD*, BD?, BrownD*, BrownD?, BrownD*_Candidate, or PM*.\n",
    "- For Gaia, the criteria for matching is that there needs to be a source within a field of view (centered on the subject's coordinates) of 120 arcseconds, plus some extra FOV to account for high proper motion objects, which has a total proper motion of 100 mas/yr or more.\n",
    "\n",
    "The remaining acceptable candidates which were not found in SIMBAD, Gaia, or either of them, will be placed into their respective csv files for manual examination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb27f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the subject IDs for the queries.\n",
    "subject_ids = analyzer.getSubjectIDs()\n",
    "\n",
    "# Get the subject type for a specific subject.\n",
    "subject_type = analyzer.getSubjectType(subject_ids[0])\n",
    "print(f\"Subject type for subject {subject_ids[0]}: {subject_type}\\n\")\n",
    "\n",
    "# Check if a specific subject is an acceptable candidate.\n",
    "is_acceptable_candidate, subject_classifications = analyzer.checkIfCandidateIsAcceptable(subject_ids[0], 0.5, acceptance_threshold=1, weighted=False)\n",
    "print(f\"Subject {subject_ids[0]} an acceptable candidate: {is_acceptable_candidate}\")\n",
    "print(f\"Subject classifications for subject {subject_ids[0]}: {subject_classifications}\\n\")\n",
    "\n",
    "# Find the acceptable candidates.\n",
    "# Saves the acceptable candidates to a csv file.\n",
    "acceptable_candidates = analyzer.findAcceptableCandidates(acceptance_ratio=0.5, save=True, weighted=False)\n",
    "print(f\"Acceptable candidates: {acceptable_candidates}\\n\")\n",
    "\n",
    "# Sort and exclude the acceptable candidates by database.\n",
    "print(\"Warning: This may take a while...\\n\")\n",
    "generated_files = analyzer.sortAcceptableCandidatesByDatabase(acceptable_candidates)\n",
    "print(f\"Generated files: {generated_files}\\n\")\n",
    "\n",
    "# To perform both the acceptable candidate finding and sorting in one step, uncomment and run the following function on its own:\n",
    "print(\"Warning: This may take a while...\\n\")\n",
    "#analyzer.performCandidatesSort(acceptance_ratio=0.5)\n",
    "print(\"Acceptable candidates found and sorted!\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc8a4da",
   "metadata": {},
   "source": [
    "## Using the Classifier\n",
    "The classifier handles user performance/accuracy in regards to classifications in the Analyzer object. The primary utility of the classifier is already built into the Analyzer object via the weighted keyword in the candidate functions, but the class itself has other functionality for detailing information about user accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c72bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the classifier object from within the analyzer object.\n",
    "classifier = analyzer.classifier\n",
    "\n",
    "# Get the usernames from the analyzer.\n",
    "usernames = analyzer.getUniqueUserIdentifiers(user_identifier=\"username\", include_logged_out_users=True)\n",
    "\n",
    "# Get the user accuracy for a specific user.\n",
    "user_accuracy = classifier.getUserAccuracy(usernames[0], default_insufficient_classifications=True)\n",
    "print(f\"User accuracy for user {usernames[0]}: {user_accuracy}\\n\")\n",
    "\n",
    "# Get the user verified classifications for a specific user.\n",
    "verified_classifications_by_user = classifier.getUserVerifiedClassifications(usernames[0])\n",
    "print(f\"Verified classifications by user {usernames[0]}: {verified_classifications_by_user}\\n\")\n",
    "\n",
    "# Get the user information for a specific user.\n",
    "user_information = classifier.getUserInformation(usernames[0], default_insufficient_classifications=True)\n",
    "print(f\"User information for user {usernames[0]}: {user_information}\\n\")\n",
    "\n",
    "# Get the user accuracy for all users.\n",
    "user_accuracies = classifier.getAllUserAccuracies(include_logged_out_users=True, default_insufficient_classifications=True)\n",
    "print(\"User accuracies:\", *user_accuracies[0:10], \"...\\n\")\n",
    "\n",
    "# Get all user information.\n",
    "user_information = classifier.getAllUserInformation(include_logged_out_users=True, default_insufficient_classifications=True)\n",
    "print(f\"User information: too much to display...\\n\")\n",
    "\n",
    "# Get the most accurate users.\n",
    "most_accurate_users = classifier.getMostAccurateUsernames(include_logged_out_users=True, default_insufficient_classifications=True, classification_threshold=0, verified_classifications_threshold=10, accuracy_threshold=0.0)\n",
    "print(\"Most accurate users:\", *most_accurate_users[0:10], \"...\\n\")\n",
    "\n",
    "# Plot user performance.\n",
    "classifier.plotUserPerformance(usernames[0])\n",
    "\n",
    "# Plot all users' performance as a histogram.\n",
    "classifier.plotAllUsersPerformanceHistogram(include_logged_out_users=True, default_insufficient_classifications=True)\n",
    "\n",
    "# Plot top users' performance as a histogram.\n",
    "classifier.plotTopUsersPerformanceHistogram(classification_threshold=None, percentile=98, default_insufficient_classifications=True)\n",
    "\n",
    "# Plot top users' performances\n",
    "classifier.plotTopUsersPerformances(classification_threshold=None, percentile=98, default_insufficient_classifications=True)\n",
    "\n",
    "# Plot most accurate users' performances\n",
    "classifier.plotMostAccurateUsers(include_logged_out_users=True, default_insufficient_classifications=True, classification_threshold=0, verified_classifications_threshold=100, accuracy_threshold=0.0)\n",
    "\n",
    "# Plot accuracy vs. number of classifications\n",
    "classifier.plotAccuracyVsClassificationTotals(include_logged_out_users=True, default_insufficient_classifications=True, log_plot=True, classification_threshold=0, verified_classifications_threshold=100, accuracy_threshold=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e90808",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataToolkit",
   "language": "python",
   "name": "datatoolkit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
